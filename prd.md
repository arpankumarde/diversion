# NAZITEST - Product Requirements Document
## AI-Driven Autonomous Penetration Testing Framework
### Version 1.0 | February 2026

---

**Classification:** INTERNAL - CONFIDENTIAL
**Author:** System Design Engineering, Senior Staff
**Status:** DRAFT - FOR REVIEW
**Target:** Authorized Penetration Testing & Security Research

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Problem Statement & First Principles](#2-problem-statement--first-principles)
3. [Competitive Landscape & Prior Art](#3-competitive-landscape--prior-art)
4. [System Architecture](#4-system-architecture)
5. [Core Modules](#5-core-modules)
6. [Data Pipeline & Storage](#6-data-pipeline--storage)
7. [LLM Reasoning Engine](#7-llm-reasoning-engine)
8. [Knowledge Graph Design](#8-knowledge-graph-design)
9. [Exploitation Engine](#9-exploitation-engine)
10. [Technical Specifications](#10-technical-specifications)
11. [Security, Legal & Ethics](#11-security-legal--ethics)
12. [Milestones & Phases](#12-milestones--phases)
13. [Risk Analysis](#13-risk-analysis)
14. [Appendix: Technology Decisions](#appendix-technology-decisions)

---

## 1. Executive Summary

**NAZITEST** is an AI-powered, autonomous web application penetration testing framework that combines real-time browser-based reconnaissance with LLM-driven vulnerability reasoning and automated exploitation.

### Core Thesis

Current pentesting tools fall into two camps: **dumb automation** (Nessus, ZAP scanners that fire known signatures) and **expensive human labor** (manual pentesters at $300+/hr). The gap between them -- business logic flaws, chained exploits, context-aware authentication bypasses -- is where 82% of real-world breaches occur (Verizon DBIR 2025). NAZITEST fills this gap by combining:

1. **Real browser reconnaissance** via CDP -- not synthetic HTTP requests, but actual browser sessions that render JS, handle CSP, execute AJAX, and see the app as a user does
2. **Full-spectrum recording** -- every network request, response, cookie, header, WebSocket frame, and DOM mutation gets captured as structured HAR + JSON artifacts
3. **LLM reasoning over a knowledge graph** -- a local model (via OpenRouter) builds a relational model of the target's attack surface, cross-references with optional source code, and generates ranked vulnerability hypotheses
4. **Autonomous exploitation** -- confirmed hypotheses are tested using `curl_cffi` (TLS-fingerprint-safe HTTP), browser-based replay, and adaptive strategy rotation

### What This Is NOT

- Not a vulnerability scanner (no signature database)
- Not a replacement for human pentesters (it's a force multiplier)
- Not a tool for unauthorized testing (requires explicit scope authorization)

---

## 2. Problem Statement & First Principles

### First Principles Decomposition

**Q: What does a pentester actually do?**

Decomposed into irreducible primitives:

```
PENTESTER_WORKFLOW = {
    "recon":       "Observe the system as a user would",
    "model":       "Build a mental model of how it works",
    "hypothesize": "Generate theories about what could break",
    "validate":    "Test theories against the live system",
    "adapt":       "When blocked, change strategy",
    "chain":       "Combine small findings into larger exploits",
    "document":    "Record everything for reproducibility"
}
```

**Q: Why can't existing tools do this?**

| Primitive | Burp/ZAP | Nuclei | Manual Pentester | NAZITEST |
|-----------|----------|--------|-----------------|----------|
| Real browser recon | Proxy-based (partial) | HTTP only | Yes | Yes (CDP) |
| Mental model | None | Template matching | Yes (brain) | Knowledge graph |
| Hypothesis generation | Rule-based | Signature-based | Creative reasoning | LLM reasoning |
| Adaptive strategy | None | None | Yes | LLM + backoff loops |
| Exploit chaining | Manual | None | Yes | Graph traversal + LLM |
| Full recording | HAR export | Minimal | Screenshots + notes | HAR + DOM + video + JSON |

**Q: Why a browser instead of raw HTTP?**

First principles answer: Modern web apps are not documents -- they're applications. A `GET /dashboard` returns a shell; the actual content loads via 47 subsequent XHR calls, 3 WebSocket connections, and client-side routing. Raw HTTP tools miss:

- Client-side rendered content (React/Vue/Angular SPAs)
- WebSocket-based real-time features
- CSRF tokens generated by JS
- Anti-bot challenges (Cloudflare Turnstile, DataDome)
- Browser fingerprint-dependent behavior
- OAuth/OIDC redirect chains with JS-based state management

---

## 3. Competitive Landscape & Prior Art

### Research-Validated (Feb 2026)

| Tool | Architecture | Strengths | Weaknesses | Relevance to NAZITEST |
|------|-------------|-----------|------------|----------------------|
| **Hound** (Bernhard Mueller) | Knowledge-graph + multi-LLM agents (Scout/Strategist/Finalizer) | Adaptive knowledge graphs, belief refinement, confirmed real bugs in Rust codebase | Code-only (no runtime testing), <80k LOC limit | **Direct inspiration** for KG design and multi-agent reasoning |
| **LuaN1ao** (鸾鸟) | State-awareness + causal reasoning agent | Step-by-step pentesting, PoC generation, Ghidra integration, causal graphs | Chinese-language prompts, needs translation | **Direct inspiration** for exploitation reasoning and causal reporting |
| **Hexstrike AI** | MCP framework + 150 security tools + 12 AI agents | Fast results, tool orchestration, reproducible | Tool installation complexity, docs | Informs our tool orchestration layer |
| **CRAKEN** (OpenReview) | LLM agents + knowledge graph reasoning for threat modeling | Graph-based cybersecurity reasoning, 11 citations | Academic, not production-ready | Validates KG approach for vuln analysis |
| **AutoPen** (ACM 2025) | LLM + knowledge graph for automated pentesting | Organized info into KG for automated pentest | Early research stage | Validates core architecture thesis |
| **Raptor** | Claude Code + SemGrep + CodeQL | Fast code review, PoC generation, patch suggestions | Code-only, Claude-dependent | Informs our codebase analysis module |
| **BurpGPT** | Burp Suite extension + LLM | Integrates with existing workflow | Plugin, not autonomous | Validates LLM + proxy concept |
| **Pentestagent** | Docker-based agent framework | Clean TUI, ZAP/Nuclei/nikto orchestration | Poor strategy selection, no human-in-loop control | Informs our agent loop design |

### Key Insight from Research

> "The AI pentest tools would likely pass a CEH exam. This goes beyond Tenable Nessus or Rapid7 InsightVM vulnerability scans." -- because-security.com, Dec 2025

> "82% of exploited vulnerabilities involved human reasoning, exploit chaining, and contextual analysis -- areas where automation alone is insufficient." -- Verizon DBIR 2025

NAZITEST's differentiator: **runtime browser reconnaissance + knowledge graph reasoning + autonomous exploitation** in a single integrated pipeline. No existing tool combines all three.

---

## 4. System Architecture

### High-Level Architecture

```
                                    NAZITEST SYSTEM ARCHITECTURE
                                    ===========================

    ┌──────────────────────────────────────────────────────────────────────────┐
    │                           OPERATOR INTERFACE                            │
    │  CLI / TUI / Web Dashboard                                              │
    │  - Target URL + Scope config                                            │
    │  - Codebase path (optional)                                             │
    │  - Proxy config                                                         │
    │  - Authorization confirmation                                           │
    └─────────────────────────────┬────────────────────────────────────────────┘
                                  │
                    ┌─────────────▼──────────────┐
                    │      ORCHESTRATOR          │
                    │  (Main Control Loop)       │
                    │  - Phase management        │
                    │  - State machine           │
                    │  - Human-in-loop gates     │
                    └──┬───────┬───────┬────┬────┘
                       │       │       │    │
         ┌─────────────▼──┐ ┌──▼───────▼┐ ┌▼──────────────┐
         │   RECON ENGINE │ │ REASONING │ │  EXPLOITATION  │
         │                │ │  ENGINE   │ │    ENGINE      │
         │ ┌────────────┐ │ │           │ │                │
         │ │CDP Browser │ │ │ ┌───────┐ │ │ ┌────────────┐ │
         │ │(Zendriver/ │ │ │ │OpenR. │ │ │ │curl_cffi   │ │
         │ │ cdp-use)   │ │ │ │ LLM   │ │ │ │TLS-safe    │ │
         │ ├────────────┤ │ │ ├───────┤ │ │ ├────────────┤ │
         │ │HAR Recorder│ │ │ │Know.  │ │ │ │Browser     │ │
         │ │DOM Snapshot│ │ │ │Graph  │ │ │ │Replay      │ │
         │ │Cookie Jar  │ │ │ │Engine │ │ │ ├────────────┤ │
         │ │WS Monitor  │ │ │ ├───────┤ │ │ │Strategy    │ │
         │ └────────────┘ │ │ │Code   │ │ │ │Rotator     │ │
         │                │ │ │Xref   │ │ │ │+ Backoff   │ │
         └───────┬────────┘ │ └───────┘ │ │ └────────────┘ │
                 │          └─────┬─────┘ └───────┬────────┘
                 │                │                │
         ┌───────▼────────────────▼────────────────▼────────┐
         │              LOCAL STORAGE LAYER                  │
         │                                                   │
         │  /nazitest_runs/<run_id>/                         │
         │  ├── har/              (HAR archives)             │
         │  ├── dom_snapshots/    (DOM trees per page)       │
         │  ├── screenshots/      (timestamped PNGs)         │
         │  ├── cookies/          (cookie jars per domain)   │
         │  ├── websocket_logs/   (WS frame dumps)           │
         │  ├── site_map.json     (discovered endpoints)     │
         │  ├── knowledge_graph/  (nodes, edges, beliefs)    │
         │  ├── codebase_xref/    (if source provided)       │
         │  ├── exploits/         (PoC scripts + results)    │
         │  └── report/           (final output)             │
         └──────────────────────────────────────────────────┘
```

### Process Flow (State Machine)

```
    INIT ──► AUTHORIZE ──► CRAWL ──► RECORD ──► MODEL ──► REASON ──► EXPLOIT ──► REPORT
     │          │            │         │          │         │           │           │
     │       (confirm       (real    (HAR,DOM,  (build    (LLM      (curl_cffi, (structured
     │        scope)       browser   cookies,   knowledge  hypothe-  browser     JSON +
     │                    crawling)  JSON,WS)   graph)    size)     replay)     HTML)
     │                                                       │
     │                                                       ▼
     │                                              BACKOFF & RE-STRATEGIZE
     │                                              (if exploit blocked/fails)
     │                                                       │
     └───────────────────────────────────────────────────────┘
                          (loop until coverage threshold or time limit)
```

---

## 5. Core Modules

### 5.1 Recon Engine: CDP Browser Controller

**Technology Decision: Zendriver (primary) + cdp-use (low-level fallback)**

**Rationale (research-validated Feb 2026):**

| Tool | Anti-Bot Bypass Rate | Async | CDP Direct | Python-first |
|------|---------------------|-------|------------|-------------|
| Zendriver | **75%** (CF, CloudFront, Akamai) | Yes | Yes | Yes |
| Nodriver | 25% | Yes | Yes | Yes |
| Playwright | 25% | Yes | Via relay | No (Node.js core) |
| Selenium | 25% | No | No (WebDriver) | Yes |

Source: Dima Kynal, "Baseline Performance Comparison", Aug 2025

Zendriver is the fork of Nodriver with active development, bypasses Cloudflare/Akamai/CloudFront out-of-box. For cases needing deeper CDP control (custom frame routing, OOPIF handling), we use `cdp-use` (browser-use's type-safe CDP bindings, released Aug 2025).

**Key CDP Capabilities We Use:**

```python
# Network interception & HAR recording
"Network.enable"
"Network.requestWillBeSent"       # Capture all requests
"Network.responseReceived"        # Capture all responses
"Network.getResponseBody"         # Get response bodies
"Network.webSocketFrameSent"      # WebSocket monitoring
"Network.webSocketFrameReceived"

# DOM & Page state
"DOMSnapshot.captureSnapshot"     # Full DOM tree capture
"Page.captureScreenshot"          # Visual evidence
"Page.handleJavaScriptDialog"     # Handle alerts/confirms

# Storage extraction
"Network.getCookies"              # All cookies for domain
"Runtime.evaluate"                # Extract localStorage/sessionStorage
"Storage.getCookies"              # Cookie jar access

# Authentication & Session
"Network.setExtraHTTPHeaders"     # Inject auth headers
"Network.setCookie"               # Set cookies for replay
```

**Crawling Strategy:**

```python
class CrawlStrategy:
    """
    Multi-phase crawling mimicking human pentester behavior.
    """
    phases = [
        "passive_observation",    # Browse site normally, record everything
        "sitemap_discovery",      # robots.txt, sitemap.xml, common paths
        "link_extraction",        # Follow all <a>, <form>, JS routes
        "api_discovery",          # Monitor XHR/fetch calls, extract API patterns
        "auth_flow_mapping",      # Login, register, password reset flows
        "input_enumeration",      # All forms, URL params, headers that accept input
        "error_provocation",      # 404s, invalid inputs, edge cases
    ]
```

### 5.2 Recording Engine

**Everything gets saved locally. No data leaves the machine except LLM API calls (which send only structured summaries, never raw credentials).**

#### HAR Recording

Full HTTP Archive format per the W3C spec. Every request/response pair including:

```python
@dataclass
class HAREntry:
    request: {
        method: str           # GET, POST, PUT, DELETE, PATCH, OPTIONS
        url: str              # Full URL with query params
        headers: List[Dict]   # All headers including auth tokens
        cookies: List[Dict]   # Request cookies
        postData: Optional    # POST body (form, JSON, multipart)
        queryString: List     # Parsed query parameters
    }
    response: {
        status: int           # HTTP status code
        headers: List[Dict]   # Response headers (CSP, CORS, Set-Cookie, etc.)
        content: {
            mimeType: str
            text: str         # Response body
            size: int
        }
        cookies: List[Dict]   # Set-Cookie parsed
    }
    timing: {                 # Performance timing
        dns: float
        connect: float
        ssl: float
        send: float
        wait: float           # TTFB
        receive: float
    }
    serverIPAddress: str
    connection: str           # Connection ID for keep-alive tracking
```

#### DOM Snapshots

```python
@dataclass
class DOMSnapshot:
    url: str
    timestamp: float
    html: str                 # Full rendered HTML (post-JS execution)
    forms: List[FormData]     # Extracted form elements with actions, methods, inputs
    links: List[str]          # All discovered links
    scripts: List[ScriptInfo] # Inline + external scripts with SRI hashes
    meta: Dict                # Meta tags (CSP, X-Frame-Options, etc.)
    localStorage: Dict        # localStorage dump
    sessionStorage: Dict      # sessionStorage dump
    cookies: List[CookieInfo] # Full cookie jar with flags
    console_logs: List[str]   # Console output (errors, warnings)
```

#### Site Map JSON

```python
@dataclass
class SiteMap:
    endpoints: List[Endpoint]     # Discovered URL paths
    api_routes: List[APIRoute]    # XHR/fetch patterns with methods + params
    auth_flows: List[AuthFlow]    # Login/register/reset sequences
    static_assets: List[Asset]    # JS/CSS/images with hashes
    technologies: TechStack       # Detected frameworks, servers, CDNs
    security_headers: Dict        # Per-page security header analysis
    cors_policy: Dict             # CORS configuration per origin
    csp_policy: Dict              # Content Security Policy analysis
    websocket_endpoints: List     # WS connection URLs + message patterns
```

### 5.3 Proxy Integration

```python
class ProxyManager:
    """
    Manages proxy rotation for both browser and curl_cffi sessions.
    Supports: HTTP, HTTPS, SOCKS4, SOCKS5, residential rotating.
    """
    def __init__(self, proxy_config: ProxyConfig):
        self.proxies = proxy_config.proxy_list
        self.rotation_strategy = proxy_config.strategy  # round-robin, random, geo-targeted
        self.backoff_tracker = BackoffTracker()

    def get_proxy(self, purpose: str = "recon") -> Proxy:
        """
        Different proxies for different purposes:
        - recon: residential proxies (blend in)
        - exploit: datacenter proxies (speed)
        - validation: different geo proxy (confirm not geo-blocked)
        """
        ...

    def mark_burned(self, proxy: Proxy, reason: str):
        """Remove proxy from rotation after detection."""
        ...
```

---

## 6. Data Pipeline & Storage

### Local-First Architecture

All data stays on-disk. No cloud dependencies for storage. Structure:

```
nazitest_runs/
└── <run_id>/                          # UUID + timestamp
    ├── config.json                    # Run configuration (target, scope, options)
    ├── authorization.sig              # Signed authorization confirmation
    │
    ├── recon/
    │   ├── har/
    │   │   ├── 001_initial_load.har
    │   │   ├── 002_login_flow.har
    │   │   └── ...
    │   ├── dom/
    │   │   ├── page_<hash>_<ts>.json
    │   │   └── ...
    │   ├── screenshots/
    │   │   ├── <ts>_<page>.png
    │   │   └── ...
    │   ├── cookies/
    │   │   └── cookie_jar_<ts>.json
    │   ├── websockets/
    │   │   └── ws_<endpoint>_<ts>.jsonl
    │   ├── site_map.json
    │   └── tech_stack.json
    │
    ├── analysis/
    │   ├── knowledge_graph/
    │   │   ├── graph.json             # NetworkX-serialized graph
    │   │   ├── nodes.jsonl            # Node definitions
    │   │   ├── edges.jsonl            # Edge definitions
    │   │   └── beliefs.jsonl          # Vulnerability hypotheses + confidence
    │   ├── codebase_xref/             # If source code provided
    │   │   ├── ast_analysis.json
    │   │   ├── route_mapping.json     # URL <-> code handler mapping
    │   │   ├── sink_source_flows.json # Taint analysis results
    │   │   └── dependency_audit.json
    │   └── llm_reasoning/
    │       ├── session_<n>.jsonl      # LLM conversation logs
    │       ├── hypotheses.json        # Generated hypotheses
    │       └── cross_validation.json  # Cross-reasoning results
    │
    ├── exploitation/
    │   ├── attempts/
    │   │   ├── attempt_001.json       # Request, response, strategy, result
    │   │   └── ...
    │   ├── pocs/
    │   │   ├── poc_sqli_login.py
    │   │   ├── poc_xss_search.py
    │   │   └── ...
    │   └── results.json               # Exploitation outcomes
    │
    └── report/
        ├── report.json                # Machine-readable
        ├── report.html                # Human-readable with evidence
        └── executive_summary.md       # C-suite version
```

### Data Sanitization for LLM Calls

**Critical: Never send raw credentials, tokens, or PII to the LLM API.**

```python
class LLMDataSanitizer:
    """
    Strips sensitive data before sending to OpenRouter.
    The LLM sees structure and patterns, not secrets.
    """
    REDACT_PATTERNS = [
        r'(Authorization:\s*Bearer\s+)\S+',      # Auth tokens
        r'(password["\s:=]+)\S+',                  # Passwords
        r'(api[_-]?key["\s:=]+)\S+',              # API keys
        r'([A-Za-z0-9+/]{40,}={0,2})',            # Base64 blobs > 40 chars
        r'(eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+)', # JWTs (header.payload)
    ]

    def sanitize_for_llm(self, data: dict) -> dict:
        """Replace sensitive values with type descriptors."""
        # "Authorization: Bearer eyJ..." -> "Authorization: Bearer <JWT_TOKEN:len=342>"
        ...
```

---

## 7. LLM Reasoning Engine

### Model Selection via OpenRouter

**OpenRouter** provides a unified API gateway to multiple LLM providers. Based on research (Feb 2026), the recommended model configuration:

| Role | Recommended Model | Context Window | Why |
|------|------------------|----------------|-----|
| **Graph Builder** | `google/gemini-3-flash-preview` | 1.05M tokens | Largest context for ingesting full recon data. $0.50/M input. |
| **Strategist** (lead reasoning) | `anthropic/claude-opus-4.6` | 1M tokens | Strongest reasoning for hypothesis generation. |
| **Scout** (exploration) | `deepseek/deepseek-v3.2` | 164K tokens | Cost-effective ($0.25/M input) for exploratory analysis. |
| **Exploit Planner** | `anthropic/claude-sonnet-4.6` | 1M tokens | Strong agentic coding + tool use for exploit generation. |
| **Cross-Validator** | `x-ai/grok-4.1-fast` | 2M tokens | Independent model family for unbiased cross-validation. |
| **Codebase Analyzer** | `deepseek/deepseek-v3.2` | 164K tokens | Strong code understanding at low cost. |

> **Note on "Codex 5.3":** This model name does not appear in current OpenRouter listings. The architecture is model-agnostic -- any model accessible via OpenRouter's `/api/v1/chat/completions` endpoint can be swapped in. The system will support a `models.yaml` config for easy model rotation.

**OpenRouter Integration:**

```python
class OpenRouterClient:
    BASE_URL = "https://openrouter.ai/api/v1"

    def __init__(self, api_key: str, models_config: dict):
        self.api_key = api_key
        self.models = models_config
        self.usage_tracker = UsageTracker()

    async def reason(self, role: str, messages: list,
                     structured_output: dict = None) -> dict:
        """
        Send reasoning request to appropriate model based on role.
        Supports JSON mode / structured output for graph operations.
        """
        model = self.models[role]
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://nazitest.local",
            "X-Title": "NAZITEST Security Analysis"
        }

        payload = {
            "model": model["id"],
            "messages": messages,
            "temperature": model.get("temperature", 0.3),
            "max_tokens": model.get("max_tokens", 8192),
        }

        if structured_output:
            payload["response_format"] = {
                "type": "json_schema",
                "json_schema": structured_output
            }

        # ... make request, track usage, handle errors
```

### Multi-Agent Reasoning Architecture

Inspired by **Hound** (Bernhard Mueller, Sep 2025) and **LuaN1ao**:

```
                    ┌─────────────────────┐
                    │    STRATEGIST        │
                    │  (Claude Opus 4.6)   │
                    │                     │
                    │  - Reviews KG       │
                    │  - Plans audit      │
                    │  - Generates        │
                    │    hypotheses       │
                    │  - Assigns tasks    │
                    └──────┬──────────────┘
                           │
              ┌────────────┼────────────┐
              │            │            │
    ┌─────────▼──┐  ┌──────▼─────┐  ┌──▼──────────┐
    │   SCOUT 1  │  │  SCOUT 2   │  │  SCOUT 3    │
    │ (DeepSeek) │  │ (DeepSeek) │  │ (DeepSeek)  │
    │            │  │            │  │             │
    │ Auth/Authz │  │ Input Val. │  │ API/Logic   │
    │ analysis   │  │ analysis   │  │ analysis    │
    └─────┬──────┘  └──────┬─────┘  └──────┬──────┘
          │                │               │
          └────────────────┼───────────────┘
                           │
                    ┌──────▼──────────────┐
                    │  CROSS-VALIDATOR    │
                    │  (Grok 4.1 Fast)   │
                    │                     │
                    │  - Independent      │
                    │    verification     │
                    │  - Challenge        │
                    │    assumptions      │
                    │  - Adjust beliefs   │
                    └──────┬──────────────┘
                           │
                    ┌──────▼──────────────┐
                    │  EXPLOIT PLANNER   │
                    │  (Claude Sonnet)    │
                    │                     │
                    │  - PoC generation   │
                    │  - curl_cffi cmds   │
                    │  - Browser replay   │
                    │  - Backoff strategy │
                    └─────────────────────┘
```

### Reasoning Protocol

Each reasoning cycle follows this belief-refinement loop (from Hound):

```python
class BeliefRefinementLoop:
    """
    Hypotheses start at a baseline confidence and are adjusted
    as evidence accumulates. This prevents both false positives
    and missed vulnerabilities.
    """

    def cycle(self, knowledge_graph: KnowledgeGraph):
        # 1. STRATEGIST reviews current graph state
        plan = self.strategist.analyze(knowledge_graph)

        # 2. STRATEGIST generates vulnerability hypotheses
        hypotheses = self.strategist.hypothesize(plan)
        for h in hypotheses:
            h.confidence = 0.3  # Starting baseline

        # 3. SCOUTS investigate each hypothesis
        for hypothesis in hypotheses:
            evidence = self.scout.investigate(
                hypothesis,
                knowledge_graph,
                recon_data=self.recon_artifacts
            )

            # 4. Update confidence based on evidence
            hypothesis.confidence = self.update_belief(
                hypothesis.confidence,
                evidence
            )

            # 5. Annotate knowledge graph with findings
            knowledge_graph.annotate(hypothesis, evidence)

        # 6. CROSS-VALIDATOR challenges high-confidence findings
        for h in hypotheses:
            if h.confidence > 0.6:
                validation = self.cross_validator.challenge(h)
                h.confidence = self.reconcile(h.confidence, validation)

        # 7. Confirmed hypotheses (>0.75) go to exploitation
        confirmed = [h for h in hypotheses if h.confidence > 0.75]
        return confirmed
```

---

## 8. Knowledge Graph Design

### Graph Schema

Built with **NetworkX** (in-memory, serializable to JSON) for speed and simplicity. If scale demands it, can migrate to embedded Neo4j.

```python
# Node Types
NODE_TYPES = {
    "endpoint":      "URL path with method (GET /api/users)",
    "parameter":     "Input parameter (query, body, header, cookie)",
    "auth_mechanism":"Authentication method (JWT, session, API key)",
    "auth_scope":    "Authorization level (admin, user, guest)",
    "data_flow":     "Data transformation between components",
    "technology":    "Detected tech (React, Express, PostgreSQL)",
    "security_ctrl": "Security control (CSP, CORS, rate limit, WAF)",
    "vulnerability": "Hypothesized vulnerability",
    "evidence":      "Supporting evidence for a hypothesis",
    "code_handler":  "Source code function handling this endpoint (if available)",
    "cookie":        "Cookie with its flags and scope",
    "api_pattern":   "Repeated API interaction pattern",
}

# Edge Types
EDGE_TYPES = {
    "accepts_input":    ("endpoint", "parameter"),
    "authenticated_by": ("endpoint", "auth_mechanism"),
    "requires_scope":   ("endpoint", "auth_scope"),
    "data_flows_to":    ("parameter", "endpoint"),        # Taint propagation
    "protected_by":     ("endpoint", "security_ctrl"),
    "may_have_vuln":    ("endpoint", "vulnerability"),
    "supported_by":     ("evidence", "vulnerability"),
    "handled_by":       ("endpoint", "code_handler"),     # Codebase xref
    "sets_cookie":      ("endpoint", "cookie"),
    "uses_cookie":      ("endpoint", "cookie"),
    "escalates_to":     ("auth_scope", "auth_scope"),     # Priv escalation paths
}
```

### Graph Construction from Recon Data

```python
class KnowledgeGraphBuilder:
    """
    Phase 1: Automated graph construction from HAR + DOM data
    Phase 2: LLM-assisted enrichment (pattern recognition, inference)
    Phase 3: Codebase cross-reference (if source available)
    """

    def build_from_recon(self, recon_data: ReconArtifacts) -> KnowledgeGraph:
        graph = KnowledgeGraph()

        # 1. Parse HAR files -> endpoints, parameters, auth patterns
        for har in recon_data.har_files:
            for entry in har.entries:
                ep = graph.add_endpoint(entry.request.url, entry.request.method)
                for param in extract_params(entry.request):
                    p = graph.add_parameter(param.name, param.location, param.type)
                    graph.add_edge(ep, p, "accepts_input")

                # Auth detection
                auth = detect_auth(entry.request.headers, entry.request.cookies)
                if auth:
                    graph.add_edge(ep, auth, "authenticated_by")

                # Security header analysis
                sec = analyze_security_headers(entry.response.headers)
                for ctrl in sec:
                    graph.add_edge(ep, ctrl, "protected_by")

        # 2. LLM enrichment: ask model to identify patterns
        graph = self.llm_enrich(graph)

        # 3. Codebase cross-reference (if available)
        if recon_data.codebase_path:
            graph = self.crossref_codebase(graph, recon_data.codebase_path)

        return graph
```

### Codebase Cross-Reference

When source code is available, NAZITEST maps runtime observations to code:

```python
class CodebaseXRef:
    """
    Maps discovered endpoints to their code handlers.
    Identifies taint flows from user input to dangerous sinks.
    """

    def analyze(self, codebase_path: str, site_map: SiteMap) -> XRefResult:
        # 1. Parse route definitions (Express, Flask, Django, Spring, etc.)
        routes = self.parse_routes(codebase_path)

        # 2. Match runtime endpoints to code handlers
        mappings = self.match_endpoints(site_map.endpoints, routes)

        # 3. AST-based taint analysis: input -> sink
        taint_flows = self.trace_taint_flows(codebase_path, mappings)

        # 4. Dependency audit (known CVEs in dependencies)
        dep_vulns = self.audit_dependencies(codebase_path)

        # 5. LLM-assisted code review for logic flaws
        logic_review = self.llm_code_review(codebase_path, mappings)

        return XRefResult(mappings, taint_flows, dep_vulns, logic_review)

    DANGEROUS_SINKS = {
        "sql":        ["execute", "raw", "query", "cursor"],
        "command":    ["exec", "spawn", "system", "popen", "subprocess"],
        "file":       ["readFile", "writeFile", "open", "unlink"],
        "template":   ["render", "template", "eval"],
        "redirect":   ["redirect", "location", "navigate"],
        "deserialize":["deserialize", "pickle.loads", "yaml.load", "JSON.parse"],
    }
```

---

## 9. Exploitation Engine

### Architecture: Think Like a Pentester

The exploitation engine doesn't blindly fire payloads. It follows a reasoning loop:

```python
class ExploitationEngine:
    """
    For each confirmed vulnerability hypothesis:
    1. Plan the exploit (LLM generates strategy)
    2. Prepare the environment (cookies, tokens, headers)
    3. Execute with curl_cffi (TLS-safe, WAF-evasion)
    4. If blocked: backoff, re-strategize, try alternative
    5. Record everything
    """

    async def exploit(self, hypothesis: Hypothesis,
                      knowledge_graph: KnowledgeGraph,
                      session_data: SessionData) -> ExploitResult:

        for attempt in range(self.max_attempts):
            # 1. LLM generates exploit strategy
            strategy = await self.exploit_planner.plan(
                hypothesis=hypothesis,
                previous_attempts=self.attempts,
                knowledge_graph=knowledge_graph,
                available_cookies=session_data.cookies,
                available_tokens=session_data.auth_tokens,
            )

            # 2. Execute based on strategy type
            if strategy.method == "curl_cffi":
                result = await self.execute_curl(strategy)
            elif strategy.method == "browser_replay":
                result = await self.execute_browser(strategy)
            elif strategy.method == "combined":
                result = await self.execute_combined(strategy)

            # 3. Evaluate result
            if result.success:
                return ExploitResult(
                    confirmed=True,
                    evidence=result,
                    poc=strategy.to_poc_script()
                )

            # 4. Backoff and re-strategize if blocked
            if result.blocked:
                await self.backoff(attempt)
                self.attempts.append(FailedAttempt(strategy, result))
                continue  # LLM will see previous failures and adapt

            # 5. Not blocked but didn't work -> vuln may not exist
            if result.inconclusive:
                hypothesis.confidence *= 0.7
                break

        return ExploitResult(confirmed=False, attempts=self.attempts)
```

### curl_cffi Integration

**Why curl_cffi (research-validated):**

- Impersonates browser TLS fingerprints (JA3/JA4) -- WAFs that fingerprint TLS won't flag it as a script
- Supports HTTP/2 and HTTP/3 -- matches real browser behavior
- Async support with proxy rotation per request
- `requests`-compatible API -- easy to generate from LLM
- Much faster than `requests`/`httpx` for bulk exploitation

```python
from curl_cffi.requests import AsyncSession

class CurlExploiter:
    """
    TLS-fingerprint-safe HTTP client for exploitation.
    Impersonates real browser TLS signatures to avoid WAF detection.
    """

    async def execute(self, strategy: ExploitStrategy,
                      session_data: SessionData) -> ExploitResponse:

        async with AsyncSession(
            impersonate="chrome136",       # Match target browser TLS fingerprint
            proxy=self.proxy_manager.get_proxy("exploit"),
            verify=True,
        ) as session:
            # Load cookies from browser session
            for cookie in session_data.cookies:
                session.cookies.set(
                    cookie.name, cookie.value,
                    domain=cookie.domain, path=cookie.path
                )

            # Execute the exploit request
            response = await session.request(
                method=strategy.method,
                url=strategy.url,
                headers=strategy.headers,
                data=strategy.body,
                params=strategy.params,
                allow_redirects=strategy.follow_redirects,
                timeout=strategy.timeout,
            )

            return ExploitResponse(
                status=response.status_code,
                headers=dict(response.headers),
                body=response.text,
                elapsed=response.elapsed,
                tls_version=response.http_version,
            )
```

### Backoff & Strategy Rotation

```python
class StrategyRotator:
    """
    When an exploit attempt is blocked, the system backs off
    and tries a different approach -- like a real pentester would.
    """

    STRATEGIES = {
        "encoding_rotation": [
            "url_encode",           # %27 instead of '
            "double_url_encode",    # %2527
            "unicode_encode",       # \u0027
            "html_encode",          # &#39;
            "hex_encode",           # \x27
            "mixed_case",           # SeLeCt instead of SELECT
        ],
        "delivery_rotation": [
            "query_param",          # ?id=payload
            "path_segment",         # /api/payload/resource
            "post_body_json",       # {"field": "payload"}
            "post_body_form",       # field=payload
            "header_injection",     # X-Custom: payload
            "cookie_injection",     # cookie=payload
            "fragment",             # #payload (client-side)
        ],
        "timing_rotation": [
            "immediate",
            "delayed_1s",
            "delayed_5s",
            "delayed_random",
            "burst_then_pause",
        ],
        "identity_rotation": [
            "rotate_proxy",
            "rotate_user_agent",
            "rotate_tls_fingerprint",  # curl_cffi impersonate targets
            "rotate_accept_language",
        ],
    }

    async def backoff(self, attempt_number: int):
        """Exponential backoff with jitter."""
        base_delay = min(2 ** attempt_number, 60)
        jitter = random.uniform(0, base_delay * 0.5)
        await asyncio.sleep(base_delay + jitter)
```

---

## 10. Technical Specifications

### Tech Stack

| Layer | Technology | Version | Justification |
|-------|-----------|---------|---------------|
| **Language** | Python | 3.11+ | Async ecosystem, security tool ecosystem, LLM SDK support |
| **Browser Automation** | Zendriver + cdp-use | Latest | Best anti-bot bypass (75%), pure CDP, async-first |
| **HTTP Client** | curl_cffi | 0.11+ | TLS fingerprint impersonation, HTTP/2+3, async |
| **LLM Gateway** | OpenRouter API | v1 | Multi-model access, pay-per-token, model routing |
| **Knowledge Graph** | NetworkX | 3.x | In-memory, fast, JSON-serializable, no DB dependency |
| **Codebase Parsing** | tree-sitter | Latest | Multi-language AST parsing (JS, Python, Go, Java, etc.) |
| **HAR Parsing** | haralyzer / custom | - | W3C HAR spec compliance |
| **CLI Framework** | Typer + Rich | Latest | Modern CLI with progress bars, tables, TUI |
| **Async Runtime** | asyncio + uvloop | Latest | High-performance event loop |
| **Data Serialization** | orjson | Latest | Fast JSON for large HAR/DOM dumps |
| **Proxy Support** | Built-in | - | HTTP/HTTPS/SOCKS4/SOCKS5 rotation |
| **Reporting** | Jinja2 + WeasyPrint | Latest | HTML/PDF report generation |

### System Requirements

```
Minimum:
  - Python 3.11+
  - 8GB RAM (16GB recommended for large sites)
  - Chrome/Chromium installed
  - OpenRouter API key
  - Internet connection (for target + LLM API)

Recommended:
  - 32GB RAM (for large knowledge graphs + codebase analysis)
  - SSD storage (HAR files can be large)
  - Residential proxy list (for stealth)
  - GPU not required (all inference via API)
```

### API Design (Internal)

```python
# Main entry point
class Nazitest:
    async def run(
        self,
        target_url: str,
        scope: ScopeConfig,              # Allowed domains, paths, methods
        proxy_config: ProxyConfig = None,
        codebase_path: str = None,
        models_config: str = "models.yaml",
        max_crawl_depth: int = 5,
        max_crawl_pages: int = 200,
        time_limit_minutes: int = 120,
        exploit_mode: str = "confirm",    # "confirm" | "safe" | "aggressive"
        human_in_loop: bool = True,       # Pause before exploitation
        output_dir: str = "./nazitest_runs",
    ) -> RunResult:
        ...
```

### CLI Interface

```bash
# Basic scan
nazitest scan https://target.example.com

# With codebase
nazitest scan https://target.example.com --codebase ./src

# With proxy
nazitest scan https://target.example.com --proxy socks5://127.0.0.1:9050

# With custom models
nazitest scan https://target.example.com --models models.yaml

# Resume interrupted scan
nazitest resume <run_id>

# View results
nazitest report <run_id> --format html

# Export knowledge graph
nazitest graph <run_id> --export graph.html
```

---

## 11. Security, Legal & Ethics

### Authorization Framework

**NAZITEST will NOT operate without explicit authorization confirmation.**

```python
class AuthorizationGate:
    """
    Before any network activity, the operator must confirm:
    1. Written authorization to test the target
    2. Scope boundaries (domains, paths, methods)
    3. Rate limits and time windows
    4. Data handling agreements
    """

    def require_authorization(self, target: str, scope: ScopeConfig):
        print(f"""
        ╔══════════════════════════════════════════════════════╗
        ║           AUTHORIZATION REQUIRED                     ║
        ╠══════════════════════════════════════════════════════╣
        ║                                                      ║
        ║  Target: {target}
        ║  Scope:  {scope.summary()}
        ║                                                      ║
        ║  By proceeding, you confirm:                         ║
        ║  [ ] Written authorization from asset owner          ║
        ║  [ ] Testing is within agreed scope                  ║
        ║  [ ] You accept responsibility for all actions       ║
        ║                                                      ║
        ║  UNAUTHORIZED TESTING IS ILLEGAL                     ║
        ╚══════════════════════════════════════════════════════╝
        """)

        confirmation = input("Type 'AUTHORIZED' to proceed: ")
        if confirmation != "AUTHORIZED":
            sys.exit("Authorization not confirmed. Exiting.")

        # Log authorization with timestamp
        self.log_authorization(target, scope, timestamp=time.time())
```

### Safety Controls

```python
SAFETY_CONTROLS = {
    # Rate limiting to prevent DoS
    "max_requests_per_second": 10,
    "max_concurrent_connections": 5,

    # Scope enforcement
    "strict_scope": True,              # Never leave target domain
    "block_destructive_methods": True, # No DELETE/PUT by default in safe mode
    "respect_robots_txt": False,       # Pentesters don't respect robots.txt

    # Data protection
    "never_exfiltrate_data": True,     # Confirm vulns exist, don't steal data
    "sanitize_llm_inputs": True,       # Strip credentials before LLM calls
    "encrypt_local_storage": True,     # AES-256 for stored artifacts

    # Kill switch
    "max_run_time_hours": 24,
    "human_approval_before_exploit": True,
    "emergency_stop_key": "Ctrl+C",
}
```

### Legal Compliance Checklist

- [ ] CFAA (US) -- Computer Fraud and Abuse Act compliance
- [ ] CMA (UK) -- Computer Misuse Act compliance
- [ ] GDPR (EU) -- Data handling during testing
- [ ] PCI DSS -- If testing payment card environments
- [ ] Bug bounty program terms -- If testing under bounty
- [ ] Penetration Testing Rules of Engagement (RoE) signed
- [ ] Third-party notification (cloud providers, CDNs)

---

## 12. Milestones & Phases

### Phase 1: Foundation (Weeks 1-4)

**Deliverable: Working recon engine with full recording**

- [ ] Project scaffolding (Python 3.11+, pyproject.toml, CI)
- [ ] Zendriver integration with CDP event subscription
- [ ] HAR recording engine (full request/response capture)
- [ ] DOM snapshot engine (rendered HTML + form extraction)
- [ ] Cookie/localStorage/sessionStorage extraction
- [ ] WebSocket frame capture
- [ ] Site map builder (endpoint + API route discovery)
- [ ] Screenshot capture with timestamps
- [ ] Local file storage with run management
- [ ] CLI framework (Typer + Rich)

### Phase 2: Intelligence (Weeks 5-8)

**Deliverable: Knowledge graph construction + LLM reasoning**

- [ ] NetworkX knowledge graph schema implementation
- [ ] Automated graph construction from recon artifacts
- [ ] OpenRouter client with multi-model support
- [ ] LLM data sanitization layer
- [ ] Strategist agent (hypothesis generation)
- [ ] Scout agents (evidence gathering)
- [ ] Belief refinement loop
- [ ] Cross-validation with independent model
- [ ] Knowledge graph visualization (HTML export)

### Phase 3: Codebase Analysis (Weeks 9-10)

**Deliverable: Source code cross-referencing**

- [ ] tree-sitter multi-language parser integration
- [ ] Route definition extraction (Express, Flask, Django, Spring, etc.)
- [ ] Runtime-to-code endpoint mapping
- [ ] Taint flow analysis (source -> sink)
- [ ] Dependency vulnerability audit
- [ ] LLM-assisted logic review
- [ ] Knowledge graph enrichment with code nodes

### Phase 4: Exploitation (Weeks 11-14)

**Deliverable: Autonomous exploitation with strategy rotation**

- [ ] curl_cffi integration with TLS impersonation
- [ ] Cookie/token replay from browser sessions
- [ ] LLM-driven exploit strategy generation
- [ ] Payload encoding rotation
- [ ] Delivery method rotation
- [ ] Proxy + identity rotation
- [ ] Exponential backoff with jitter
- [ ] Browser-based exploit replay (for client-side vulns)
- [ ] PoC script generation
- [ ] Human-in-loop approval gate

### Phase 5: Reporting & Polish (Weeks 15-16)

**Deliverable: Production-ready tool with reporting**

- [ ] Structured JSON report generation
- [ ] HTML report with embedded evidence
- [ ] Executive summary generation (LLM)
- [ ] CVSS score estimation per finding
- [ ] MITRE ATT&CK mapping
- [ ] CWE classification
- [ ] Remediation recommendations
- [ ] Run resume/replay capability
- [ ] Documentation and usage guides

---

## 13. Risk Analysis

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| LLM hallucinated vulnerabilities (false positives) | HIGH | MEDIUM | Cross-validation with independent model, exploitation confirmation required |
| Target WAF blocks all exploitation attempts | MEDIUM | HIGH | TLS fingerprint rotation via curl_cffi, proxy rotation, encoding rotation, browser fallback |
| LLM API rate limits / downtime | MEDIUM | MEDIUM | Multi-provider via OpenRouter, local model fallback, async queue with retry |
| Accidental scope violation | LOW | CRITICAL | Strict scope enforcement in code, domain whitelist, human-in-loop gates |
| Large site exceeds context window | HIGH | MEDIUM | Chunked processing, knowledge graph summarization, focused crawling |
| Anti-bot detection blocks browser | MEDIUM | HIGH | Zendriver (75% bypass), residential proxies, human-like interaction delays |
| Legal liability from aggressive testing | LOW | CRITICAL | Authorization gate, scope controls, safe mode defaults, rate limiting |
| Cost overrun on LLM API | MEDIUM | LOW | Usage tracking, budget caps, cheaper models for scout work, caching |

---

## Appendix: Technology Decisions

### Why Zendriver over Playwright for Browser Automation

**Decision Date:** Feb 2026
**Decision:** Zendriver (primary) + cdp-use (low-level)

**Evidence:**
- Zendriver bypasses Cloudflare, CloudFront, and Akamai out-of-box (75% success vs 25% for Playwright) -- Dima Kynal benchmark, Aug 2025
- Direct CDP communication (no Node.js relay server = lower latency)
- Async-first Python native
- browser-use (79K GitHub stars) migrated from Playwright to raw CDP in Aug 2025, citing: "the double RPC through the node.js relay means some state inevitably drifts" and "fullPage=True screenshot on pages longer than >16,000px high reliably crashes playwright"

**Trade-off:** Less mature ecosystem than Playwright, smaller community. Acceptable for a security tool where stealth matters more than ecosystem.

### Why curl_cffi over requests/httpx for Exploitation

**Decision Date:** Feb 2026
**Decision:** curl_cffi

**Evidence:**
- Only Python HTTP library that impersonates browser TLS fingerprints (JA3/JA4)
- Supports HTTP/2 and HTTP/3 (requests does not)
- Async with proxy rotation per request
- Performance on par with aiohttp (benchmarked)
- Active development, commercial support available via impersonate.pro

**Trade-off:** Binary dependency (pre-compiled libcurl-impersonate). Acceptable -- wheels available for all platforms.

### Why NetworkX over Neo4j for Knowledge Graph

**Decision Date:** Feb 2026
**Decision:** NetworkX (with migration path to Neo4j)

**Evidence:**
- Zero infrastructure (no database server)
- JSON-serializable (fits local-first architecture)
- Fast for graphs under 100K nodes (typical web app has <10K endpoints)
- Hound (Bernhard Mueller) successfully uses in-memory graphs for code auditing
- LuaN1ao uses causal graphs for pentest reasoning

**Migration path:** If graph exceeds 100K nodes, switch to Neo4j embedded or KuzuDB (also embedded).

### Why OpenRouter over Direct API

**Decision Date:** Feb 2026
**Decision:** OpenRouter

**Evidence:**
- Single API for Claude, GPT, Gemini, DeepSeek, Grok, and 200+ other models
- Cross-validation requires different model families (prevents systematic bias)
- Pay-per-token with no commitment
- Fallback routing if one provider is down
- 100T+ tokens served (proven infrastructure, State of AI 2025 report)

**Trade-off:** Additional latency (~50ms) vs direct API. Acceptable for reasoning tasks (not real-time).

---

## Appendix: OWASP Top 10 Mapping

NAZITEST's knowledge graph and reasoning engine specifically target:

| OWASP Category | KG Node Pattern | Detection Method |
|---------------|----------------|-----------------|
| A01: Broken Access Control | `auth_scope` edges missing or misconfigured | Test endpoints with different auth levels |
| A02: Cryptographic Failures | `security_ctrl` nodes for TLS, hashing | Header analysis, cookie flag inspection |
| A03: Injection | `parameter` -> `data_flow` -> dangerous sink | Taint analysis + payload testing |
| A04: Insecure Design | Missing `security_ctrl` edges | LLM reasoning over business logic |
| A05: Security Misconfiguration | `security_headers` analysis | Automated header/response analysis |
| A06: Vulnerable Components | `technology` nodes + CVE lookup | Dependency audit, version detection |
| A07: Auth Failures | `auth_mechanism` + `auth_flow` analysis | Session management testing, brute force detection |
| A08: Software/Data Integrity | Missing SRI, unsafe deserialization | Script integrity checks, input type analysis |
| A09: Logging Failures | Absence of audit trails | Error response analysis, timing attacks |
| A10: SSRF | Server-side URL parameters | URL parameter manipulation testing |

---

*This document is a living specification. Architecture decisions should be revisited as the underlying technologies evolve. All testing must comply with applicable laws and written authorization requirements.*
